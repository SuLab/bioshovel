#!/usr/bin/env python3

# get_plos_articles.py

# For downloading PLOS articles in XML format
# from lists generated by get_plos_lists.py

# usage: 
# cd downloaders && python3 get_plos_articles.py

import logging
import os
import re
import sys
from bs4 import BeautifulSoup
from collections import defaultdict
from glob import glob
from fetch_page import fetch_page
from random import random
from time import sleep

def parse_filename(filename):

    ''' Extracts journal name and timestamp from filename

        output:
        {'journal': [journalname], 
         'timestamp': [timestamp],
         'filename': filename}

        (returns None if no match)
    '''

    # extract 'plosgenetics' and '03-07-2016_22-56-41' from:
    # 'plosgenetics_dois_03-07-2016_22-56-41.txt'
    regex = re.compile(r'(^.+)(_dois_)(.+)(.txt$)')
    match = re.match(regex, filename)
    if match:
        return {'journal': match.group(1),
                'timestamp': match.group(3),
                'filename': filename}
    else:
        return None

def get_timestamp_groups(doi_list_files):

    ''' Identifies different timestamps in doi_list_files

        output:
        {[timestamp1]: [DOIfile1, DOIfile2, ...],
         [timestamp2]: [DOIfile3, DOIfile4, ...]}
    '''

    doi_file_dict = defaultdict(list)
    for list_filename in doi_list_files:

        match = parse_filename(list_filename)

        if match:
            extracted_timestamp = match['timestamp']
            doi_file_dict[extracted_timestamp].append(match['filename'])
        else:
            continue

    return doi_file_dict

def get_dois(doi_file, unique=False):

    ''' return a list of DOIs (unique or not)
        from a file with filename doi_file

        (file should have one DOI per line)
    '''

    with open(doi_file) as f:
        if unique:
            return list(set(line.rstrip('\n') for line in f.readlines()))
        else:
            return list(line.rstrip('\n') for line in f.readlines())

def count_total_unique_dois(doi_file_dict):

    ''' Given a dict of DOI-containing files, 
        return a dict with the same keys, but with
        the numbers of total and unique DOIs per file
    '''

    doi_file_dict_counts = defaultdict(list)

    for timestamp in doi_file_dict:
        for filename in doi_file_dict[timestamp]:
            count_dict = {'filename': filename,
                          'total': len(get_dois(filename)),
                          'unique': len(get_dois(filename, unique=True))
                          }
            doi_file_dict_counts[timestamp].append(count_dict)

    return doi_file_dict_counts

def save_documents(base_url, doi_list, data_dir):

    ''' save XML documents from the web to data_dir

        input:

        base_url is a format string of this form:
        'http://journals.plos.org/{abbrev}/article/asset?id={doi}.XML'

        doi_list is a list of DOIs

        data_dir is a writable path for saving data files
    '''

    for doi in doi_list:
        # check for plos identifier
        if '10.1371' not in doi:
            logging.warning('Skipped malformed DOI: {}'.format(doi))
            continue
        enc_doi = doi.replace('/', '%2F')
        full_url = base_url.format(doi=enc_doi)
        error, result = fetch_page(full_url, MAX_RETRIES=10, RETRY_WAIT_TIME=1)
        if error:
            logging.critical('Error retrieving {}'.format(full_url))
        else:
            doi_filename = os.path.join(data_dir, enc_doi)
            with open(doi_filename, 'w') as f:
                soup = BeautifulSoup(result, 'html.parser')
                f.write(soup.prettify())
        sleep(random()*5)

def main():

    doi_list_files = glob('plos*.txt')
    timestamp_groups = get_timestamp_groups(doi_list_files)
    doi_file_dict_counts = count_total_unique_dois(timestamp_groups)

    # choose most recent DOI list group (by timestamp)
    chosen_timestamp = sorted(doi_file_dict_counts.keys())[-1]

    # make sure log and data directories exist (create if they don't)
    log_dir = '../../logs'
    data_dir = '../../data'
    for dirname in (log_dir, data_dir):
        if not os.path.isdir(dirname):
            os.makedirs(dirname)

    log_filename = os.path.join(log_dir, 'get_plos_articles_{}.log'.format(chosen_timestamp))

    # using WARNING+ here because otherwise requests log every connection request...
    logging.basicConfig(filename=log_filename, level=logging.WARNING)
    logging.warning('Log file {}'.format(log_filename))
    logging.warning('Chosen DOI list group: {}'.format(chosen_timestamp))

    for timestamp in doi_file_dict_counts:
        logging.warning('DOI lists with timestamp: {}'.format(timestamp))
        for file_count_dict in doi_file_dict_counts[timestamp]:
            logging.warning(str(file_count_dict))

    # sample URLs:
    # http://journals.plos.org/plosone/article/asset?id=10.1371%2Fjournal.pone.0144197.XML
    # http://journals.plos.org/plosbiology/article/asset?id=10.1371%2Fjournal.pbio.1002391.XML
    # http://journals.plos.org/ploscompbiol/article/asset?id=10.1371%2Fjournal.pcbi.1004787.XML
    # http://journals.plos.org/plosgenetics/article/asset?id=10.1371%2Fjournal.pgen.1005903.XML
    # http://journals.plos.org/plosmedicine/article/asset?id=10.1371%2Fjournal.pmed.1001971.XML
    # http://journals.plos.org/plosntds/article/asset?id=10.1371%2Fjournal.pntd.0004429.XML
    # http://journals.plos.org/plospathogens/article/asset?id=10.1371%2Fjournal.ppat.1005499.XML
    base_url = 'http://journals.plos.org/{abbrev}/article/asset?id={doi}.XML'

    for doi_list_file in timestamp_groups[chosen_timestamp]:
        parsed = parse_filename(doi_list_file)
        unique_dois = get_dois(doi_list_file, unique=True)
        logging.warning('Accessing {} unique DOIs in file {}'.format(len(unique_dois), doi_list_file))
        save_documents(base_url.format(abbrev=parsed['journal'], doi='{doi}'), unique_dois, data_dir)

    logging.warning('All done')

if __name__ == '__main__':
    main()